{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "texture.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWtvMF3jScIipOnrGoRAGX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjiw/linjiw.github.io/blob/main/texture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "94zVjFposXQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d1d800-29c6-4984-bb97-d10db54503ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/poincloud/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ise4BQBDtHt",
        "outputId": "de0fd2fb-60a2-449c-d20a-6faae6be8c42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/poincloud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "ZTtqJnPuDw86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LIB_PTH = \"dtd/\"\n",
        "img = \"images\"\n",
        "labels = \"labels\""
      ],
      "metadata": {
        "id": "AzAaB-aItcHL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "# from torchvision import transforms"
      ],
      "metadata": {
        "id": "t9fF-2_wvSHa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getmp(LIB_PTH,img):\n",
        "  kind = os.listdir(os.path.join(LIB_PTH,img))\n",
        "  mp = {}\n",
        "  count = 0\n",
        "  for i in kind:\n",
        "    \n",
        "    mp[i] = count\n",
        "    count+=1\n",
        "  # print(count)\n",
        "  return mp\n",
        "mp = getmp(LIB_PTH,img)"
      ],
      "metadata": {
        "id": "CB01fqm6vf5g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getcsv(LIB_PTH,img,mp):\n",
        "  filename = \"train_file.csv\"\n",
        "  test_name = \"test_file.csv\"\n",
        "  file1 = open(filename,\"w\")\n",
        "  file1.write(f\"label,pth\\n\")\n",
        "  file2 = open(test_name,\"w\")\n",
        "  file2.write(f\"label,pth\\n\")\n",
        "  test_num = 12\n",
        "  for i in os.listdir(os.path.join(LIB_PTH,img)):\n",
        "    label = mp[i]\n",
        "    count =0\n",
        "    # test_num = 12\n",
        "\n",
        "    for idx, j in enumerate(os.listdir(os.path.join(LIB_PTH,img,i))):\n",
        "      if (j.endswith('.jpg') or j.endswith('.png') or j.endswith('.jpeg')) and idx>=test_num:\n",
        "        file1.write(f\"{label},{os.path.join(img,i,j)}\\n\")\n",
        "        count+=1\n",
        "      elif (j.endswith('.jpg') or j.endswith('.png') or j.endswith('.jpeg')):\n",
        "        file2.write(f\"{label},{os.path.join(img,i,j)}\\n\")\n",
        "    # print(count)\n",
        "  file1.close()\n",
        "  file2.close()\n",
        "  return filename\n",
        "annotation = getcsv(LIB_PTH,img,mp)\n",
        "\n"
      ],
      "metadata": {
        "id": "o9_3uTGpCZsp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir=None, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 1])\n",
        "        # img_path = self.img_labels.iloc[idx, 1]\n",
        "        image = read_image(img_path)\n",
        "        # print(image.shape)\n",
        "        label = self.img_labels.iloc[idx, 0]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            # print()\n",
        "        if self.target_transform:\n",
        "            label = self.target\n",
        "        label = int(label)\n",
        "        # print(type(label))\n",
        "        return image,label"
      ],
      "metadata": {
        "id": "m7TxaOaGuylF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_kl_loss(p, q, pad_mask=None):\n",
        "    \n",
        "    p_loss = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(p, dim=-1), torch.nn.functional.softmax(q, dim=-1), reduction='none')\n",
        "    q_loss = torch.nn.functional.kl_div(torch.nn.functional.log_softmax(q, dim=-1), torch.nn.functional.softmax(p, dim=-1), reduction='none')\n",
        "    # p_loss = torch.nn.functional.kl_div(p,q, reduction='none')\n",
        "    # q_loss = torch.nn.functional.kl_div(q,p, reduction='none')\n",
        " \n",
        "    # pad_mask is for seq-level tasks\n",
        "    if pad_mask is not None:\n",
        "        p_loss.masked_fill_(pad_mask, 0.)\n",
        "        q_loss.masked_fill_(pad_mask, 0.)\n",
        "\n",
        "    # You can choose whether to use function \"sum\" and \"mean\" depending on your task\n",
        "    p_loss = p_loss.mean()\n",
        "    q_loss = q_loss.mean()\n",
        "\n",
        "    loss = (p_loss + q_loss) / 2\n",
        "    return loss\n",
        "def train(args, model, device, train_samples, optimizer,scheduler, criterion, epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_loader = torch.utils.data.DataLoader(train_samples, batch_size=args['batch_size'], shuffle=True)\n",
        "        # print(f\"X.shape: {X.shape}\")\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # print(f\"batch_idx: {batch_idx} data {data.shape}\")\n",
        "        data = data.to(device)\n",
        "        # print(target)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "            \n",
        "        output = model(data)\n",
        "        # logits = model(data)\n",
        "            \n",
        "        # logits2 = model(data)\n",
        "\n",
        "        #     # cross entropy loss for classifier\n",
        "        # ce_loss = 0.5 * (criterion(logits, target) + criterion(logits2, target))\n",
        "\n",
        "        # kl_loss = compute_kl_loss(logits, logits2)\n",
        "\n",
        "        #     # carefully choose hyper-parameters\n",
        "        # loss = ce_loss + 5 * kl_loss\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "    # scheduler.step()\n",
        "    return loss.cpu().detach().numpy()\n",
        "def test(args, model, device, dev_samples,criterion):\n",
        "    model.eval()\n",
        "    true_y_list = []\n",
        "    pred_y_list = []\n",
        "    with torch.no_grad():\n",
        "        # for i in range(len(dev_samples)):\n",
        "            # X, Y = dev_samples[i]\n",
        "\n",
        "            # test_items = LibriItems(X, Y, context=args['context'])\n",
        "        test_loader = torch.utils.data.DataLoader(dev_samples, batch_size=args['batch_size'], shuffle=False)\n",
        "\n",
        "        for data, true_y in test_loader:\n",
        "            data = data.to(device)\n",
        "            true_y = true_y.to(device)                \n",
        "                \n",
        "            output = model(data)\n",
        "            pred_y = torch.argmax(output, axis=1)\n",
        "                \n",
        "            loss = criterion(output, true_y)\n",
        "\n",
        "            pred_y_list.extend(pred_y.tolist())\n",
        "            true_y_list.extend(true_y.tolist())\n",
        "    train_accuracy =  accuracy_score(true_y_list, pred_y_list)\n",
        "    \n",
        "    return train_accuracy , loss.cpu().detach().numpy()\n",
        "def savemodel(model,epoch):\n",
        "    now = datetime.now()\n",
        "    name = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
        "    torch.save(model.state_dict(), f\"model_epoch_{epoch}_{name}.pth\")\n",
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    # model = Network(args['context']).to(device)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "    # model = args['model']\n",
        "    train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToPILImage(),\n",
        "    torchvision.transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "    torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=1.0),\n",
        "    torchvision.transforms.RandomRotation(degrees=(0, 180)),\n",
        "    torchvision.transforms.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n",
        "    torchvision.transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    torchvision.transforms.RandomAutocontrast(),\n",
        "    torchvision.transforms.RandomCrop((224,224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    \n",
        "    ])\n",
        "    test_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToPILImage(),\n",
        "    torchvision.transforms.RandomCrop((224,224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    \n",
        "    ])\n",
        "    num_classes = 47\n",
        "    model = models.vgg16(pretrained=False)\n",
        "    # model.fc = nn.Linear(512 , 47)\n",
        "    model.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=0.9)\n",
        "    \n",
        "    scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "    train_set = CustomImageDataset(\"train_file.csv\", img_dir=\"dtd\",transform=train_transform)\n",
        "    test_set = CustomImageDataset(\"test_file.csv\", img_dir=\"dtd\",transform=test_transform)\n",
        "    # train_set,test_set, val_set = torch.utils.data.random_split(dataset = allset,lengths = [allset.__len__()-200,100,100])\n",
        "\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # If you want to use full Dataset, please pass None to csvpath\n",
        "    # train_samples = LibriSamples(data_path = args['LIBRI_PATH'], shuffle=True, partition=\"train-clean-100\", csvpath=\"./train_filenames_subset_8192_v2.csv\")\n",
        " \n",
        "    writer = SummaryWriter()\n",
        "    for epoch in range(1, args['epoch'] + 1):\n",
        "        train_loss = train(args, model, device, train_set, optimizer,scheduler1, criterion, epoch,)\n",
        "        test_acc, test_loss = test(args, model, device, test_set,criterion)\n",
        "        print('Dev accuracy ', test_acc)\n",
        "        savemodel(model,epoch)\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/test', train_loss, epoch)\n",
        "\n",
        "        writer.add_scalar('Acc/test', test_acc, epoch)"
      ],
      "metadata": {
        "id": "1aPu6Daume2c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    args = {\n",
        "        'batch_size': 32,\n",
        "        'log_interval': 30,\n",
        "        'lr': 0.1,\n",
        "        'epoch': 300\n",
        "    }\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "ZO57E3GCpP6s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}