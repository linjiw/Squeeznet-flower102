{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Squeeznet implementation using flower102 dataset\n",
    "---\n",
    "\n",
    "author: [@linjiw](mailto:linjiw@andrew.cmu.edu) \n",
    "\n",
    "date: Nov, 18, 2021\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "# import\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from os.path import join\n",
    "import os\n",
    "from skimage import io\n",
    "from torch.utils.tensorboard import SummaryWriter # try to use tensorboard to visualize your results~\n",
    "writer = SummaryWriter() \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Take a look at the dataset.\n",
    "First let's check what dataset we're dealing with: [flower102](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/).\n",
    "\n",
    "In the flower102 website, the datas we need are the [images](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz) and the [labels](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat). Let's download them, and store them in your data folder. (you'd better create a folder called data in your current directory)\n",
    "\n",
    "In the 102flowers directory, there is a jpg folder where contains all the picture files. \n",
    "\n",
    "And the imagelabels.mat ['labels'] contains the lables from 1 to 102.\n",
    "\n",
    "However, these are not we want. In general we use a csv file, which first column contains the filepath and the second colum contains the labels index from 0.\n",
    "\n",
    "Then the problem comes out: we need to get the sorted imagepath and make the labels use index 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# This is a custom dataset function, I named it Catset.\n",
    "class Catset(Dataset):\n",
    "    def __init__(self, pth_file,label_file, root_dir, transform=None):        \n",
    "        self.files = sorted(os.listdir(pth_file)) #this give us the image names in sorted\n",
    "        label_name = scipy.io.loadmat(label_file) \n",
    "        self.labels = label_name['labels'].flatten()-1 #this make our labels index 0\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_pth = os.path.join(self.root_dir, self.files[index])\n",
    "        image = io.imread(img_pth)\n",
    "        y_label = torch.tensor(int(self.labels[index]))\n",
    "        if self.transform:\n",
    "            image =self.transform(image)\n",
    "        return (image,y_label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We also need some basic transform functions.\n",
    "We need do some transformation for our datas, we need to transfer it to tensors for our device and in most times we also need to do some normalization and resize for a better performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "# define a transform variable.\n",
    "alltransform = transforms.Compose([\n",
    "                transforms.ToPILImage(),# first transfer to PILImage to do some resize work\n",
    "                transforms.Resize((224,224)), #224 is the value that our model looking for\n",
    "\t\t\t\ttransforms.ToTensor(), # this will transfer the data to tensor\n",
    "\t\t\t\ttransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # basic normalize parameters\n",
    "\t\t\t\t])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## With these defined functions and parameters, we 're ready to make our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "all_set = Catset(\n",
    "    pth_file = \"data/102flowers/jpg\",\n",
    "    label_file = \"data/imagelabels.mat\",\n",
    "    root_dir = \"data/102flowers/jpg\",\n",
    "    transform = alltransform,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data to train and validation\n",
    "We need dataset to train our model and need validation dataset to validate our loss and accuracy value. Thus we need to split the dataset we have now."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Set a number for validation dataset, and then we use the random_split function to split our data.\n",
    "val_nums = 100 #set our validation dataset numbers to 100, make sure this number is less than the total dataset capacity.\n",
    "train_set , val_set = torch.utils.data.random_split(all_set,[all_set.__len__()-val_nums,val_nums])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use dataloader to load data easily.\n",
    "pytorch has builtin Dataloader functions, which could bring use easy to set batch, shuffle and load data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# dataloader\n",
    "batch_size = 32\n",
    "dataloader = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True), #for train we need to set shuffle=True to make it randomly.\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=False), #for test we just leave it as False.\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build our Squeeznet model.\n",
    "Here we follow the [Squeeznet](https://arxiv.org/pdf/1602.07360.pdf) [v1.1](https://github.com/forresti/SqueezeNet/tree/master/SqueezeNet_v1.1) architecture.\n",
    "If you have previous knowledge or just want to implement Squeezenet quickly, you can directly go the following codes. Otherwise, you could take a look at the paper, it will give some intuition."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# Fire Module\n",
    "# Fire module take an input inplanes, and squeeze the inplanes using conv2d (kernel_size=1), after squeeze, it expand to expand1x1_planes and expand3x3_planes and then concatenate the resulst.\n",
    "class Fire(nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1) #squeeze\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True) # follow with a ReLu every time\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, #expand 1*1\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, #expand 3*3\n",
    "                                   kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([ # concatenate\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# conv1: 64 filters of resolution 3x3\n",
    "# pooling layers: pool_{1,3,5} (which means we do the pooling at these layers)\n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=36):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "      \n",
    "        self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2), # 1\n",
    "                nn.ReLU(inplace=True), \n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), \n",
    "                Fire(64, 16, 64, 64),# 2\n",
    "                Fire(128, 16, 64, 64),# 3\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),# 4\n",
    "                Fire(256, 32, 128, 128),# 5\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),# 6\n",
    "                Fire(384, 48, 192, 192),# 7\n",
    "                Fire(384, 64, 256, 256),# 8\n",
    "                Fire(512, 64, 256, 256),# 9\n",
    "            )\n",
    "        # Final convolution is initialized differently form the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1) #10\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(13)\n",
    "        )\n",
    "    \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal_(m.weight.data, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's train our model.\n",
    "Before we train our model, we'd better have some tool functions to write the code clearly. e.g, def train(), test(),"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(f\"train size = {size}\")\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # a = loss.cpu().detach().numpy\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    # print(f\"test size = {size}\")\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then let's set some hypeparmeters, loss function, optimizor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "# before setting these values, let's set device first.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n",
      "NVIDIA GeForce RTX 2080\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "# Then let's get our model and send it to device.\n",
    "num_class = 102 \n",
    "model = SqueezeNet(num_class).to(device)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SqueezeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Fire(\n",
      "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Fire(\n",
      "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (6): Fire(\n",
      "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): Fire(\n",
      "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (9): Fire(\n",
      "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (10): Fire(\n",
      "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): Fire(\n",
      "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): Fire(\n",
      "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (squeeze_activation): ReLU(inplace=True)\n",
      "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (expand1x1_activation): ReLU(inplace=True)\n",
      "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (expand3x3_activation): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Conv2d(512, 102, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): AvgPool2d(kernel_size=13, stride=13, padding=0)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then let's set some hype parameter and the loss function, optimizer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "# optimizor and loss func\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train(dataloader['train'], model, loss_fn, optimizer)\n",
    "    test_loss=test(dataloader['val'], model, loss_fn)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, t)\n",
    "    writer.add_scalar(\"Loss/test\", test_loss, t)\n",
    "writer.flush()\n",
    "\n",
    "print(\"Done!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check your results in tensorboard\n",
    "You are free to check your results in tensorborad.\n",
    "\n",
    "You could see two graphs, one is about the train loss and another is the train loss.\n",
    "\n",
    "If you're running in about 100+ epochs, you're about to see the test loss gets higher at some point and do not optimize. That's what we call overfitting.\n",
    "\n",
    "In your later research, if you see this happens, you could consider stopping your trainning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train loss\n",
    "![train loss](../results/Loss_train_flower102.svg?raw=true)\n",
    "### Test loss\n",
    "![test loss](../results/Loss_test_flower102.svg?raw=true)\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('linjiw_rog': conda)"
  },
  "interpreter": {
   "hash": "ecb3f5ea375cf8f605159b99e27c80e93d31cb48b8e0a69fb0b954bc1ad39839"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}