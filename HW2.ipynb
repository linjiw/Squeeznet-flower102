{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6ELXvpM4V7xeuwrIr82XH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjiw/linjiw.github.io/blob/main/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SO4df3x6Ris7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1: Bayesian Inference\n",
        "In this problem we’ll demonstrate how Bayes inference progressively updates your belief about the world using the following steps: 1) start with a prior belief, 2) perform an experiment, and 3) update your belief. Let’s see how this works. Suppose your friend has a fair coin (probability of 0.5 for head), and a biased coin with probability 0.8 for head. Based on past, you believe that the coin your friend is using is the fair coin with probability f = 0.5 (so equal probability to whether he is using the fair or the biased coin)."
      ],
      "metadata": {
        "id": "S031WV2VQshv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(a) Firstly, the friend flipped the coin and it came up head, what is the probability that he flipped the fair coin? Round to 2 decimal places."
      ],
      "metadata": {
        "id": "Wt2LWqpzRIz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P(H|F) = 0.5$\n",
        "\n",
        "$P(H|¬ F) = 0.8$\n",
        "\n",
        "$P(F) = 0.5$\n",
        "\n",
        "$P(¬ F) = 0.5$\n",
        "\n",
        "$P(F|H) = \\frac{P(H|F)P(F)}{P(H|F)P(F)+P(H|¬B)P(¬B)}$ = 0.38\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "402P6Gqg65O2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqiHz2bAQo-c",
        "outputId": "3d5cd0b5-8ab5-4c59-b4e9-4d3b6df8b6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the probability is 0.38\n"
          ]
        }
      ],
      "source": [
        "P_h_f = 0.5 #given fair, 0.5 head\n",
        "P_h_b = 0.8 #given bias, 0.8 head\n",
        "P_f = 0.5 #believe fair, 0.5\n",
        "P_b = 0.5 #believe bias, 0.5\n",
        "p_f_h = (P_h_f*P_f)/(P_h_f*P_f+P_h_b*P_b)\n",
        "p_f_h = round(p_f_h,2)\n",
        "# p_f_h = \"{:.2f}\".format(p_f_h_float)\n",
        "print(f\"the probability is {p_f_h}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(b) Your belief about the probability of the coin being fair is now updated to the posterior you just computed. Using this as your new prior, you performed another flip, and the coin came up head again. Compute the posterior probability of the coin being fair. Round to 2 decimal places. Hint: Given a coin, think about the independence of the outcomes of two (or more) consecutive flips."
      ],
      "metadata": {
        "id": "aMTunibbUTm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P(H|F) = 0.5$\n",
        "\n",
        "$P(H|¬ F) = 0.8$\n",
        "\n",
        "$P(F) = P(F|H) = 0.38$\n",
        "\n",
        "$P(¬ F) = 1-P(F) = 0.62$\n",
        "\n",
        "$P(F|H) = \\frac{P(H|F)P(F)}{P(H|F)P(F)+P(H|¬B)P(¬B)}$ = 0.28"
      ],
      "metadata": {
        "id": "LfRHukuD8I1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_f = p_f_h\n",
        "P_b = 1-P_f\n",
        "p_f_h = (P_h_f*P_f)/(P_h_f*P_f+P_h_b*P_b)\n",
        "p_f_h = round(p_f_h,2)\n",
        "print(f\"the probability is {p_f_h}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEhG1c2SUTFO",
        "outputId": "98a64b49-160d-4466-9c87-b9eb9aff30c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the probability is 0.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(c) Suppose you keep getting heads for the coin flip, after how many more flips will your belief of the coin being fair drop to below 0.05?Note that you may find the restaurant example (good vs. bad hygiene) we discussed in class to be useful for modeling the problem."
      ],
      "metadata": {
        "id": "UpnWWgd4VKhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this problem, we assume that the probability will be update each game, thus, we could write a while loop to do the work."
      ],
      "metadata": {
        "id": "6aR3i8ch8kdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "while p_f_h>0.05:\n",
        "  P_f = p_f_h\n",
        "  P_b = 1-P_f\n",
        "  p_f_h = (P_h_f*P_f)/(P_h_f*P_f+P_h_b*P_b)\n",
        "  count+=1\n",
        "print(f\"after {count} flips, the belief will fall below 0.05\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxw-DH_zVT5I",
        "outputId": "9094877a-5492-4560-9d92-56ee2a6da369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after 5 flips, the belief will fall below 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2: Fun with Linear Regression\n"
      ],
      "metadata": {
        "id": "H1C2kNYZVvbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##(a) MLE"
      ],
      "metadata": {
        "id": "d8raNk2dV5UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) likelihood of the data:\n",
        "\n",
        "$L(ω): = ∏_{i=1}^{n}P(y^{(i)}|x^{(i)},σ,ω)$\n",
        "\n",
        "$L(ω): = ∏_{i=1}^{n}\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}$\n",
        "\n",
        "$mean = 0, variance = σ$\n"
      ],
      "metadata": {
        "id": "TE9anZLOVOAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2)log-likelihood:\n",
        "\n",
        "$LL(ω):= logL(ω)$\n",
        "\n",
        "\n",
        "$LL(ω): = log∏\\limits_{i=1}\\limits^{n}\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}$\n",
        "\n",
        "$LL(ω): = \\sum\\limits_{i=1}\\limits^{n}log\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}$\n",
        "\n",
        "$LL(ω): = {n}log\\frac{1}{2πσ^2}-{\\frac{1}{2σ^2}\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}$\n",
        "\n",
        "Then we remove constant multipliers and term that don't include ω:\n",
        "\n",
        "$ω̂ = \\mathop{argmax}\\limits_{ω} - ∑\\limits_{i=1}\\limits^{m}(Y_i-\\omega X_i)^2$\n",
        "\n",
        "Since we have a negative sign inside, we can reverse it to a $argmin$ function.\n",
        "\n",
        "$ω̂ = \\mathop{argmin}\\limits_{ω}∑\\limits_{i=1}\\limits^{m}(Y_i-\\omega X_i)^2$\n",
        "\n",
        "The sum term can be rewrite using the whole vector of $X$ and $Y$. Where $X$ and $Y$ are the column vector of data sample $D = \\{(y^{i},x^{i})\\}_{i=1}^{n}$\n",
        "\n",
        "$ω̂ = \\mathop{argmin}\\limits_{ω}{||Xω-Y||}^2$\n",
        "\n",
        "Thus, by minimize this term we could maximize the likelihood $L(ω)$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X9EA6h6bZNkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##（b） MAP"
      ],
      "metadata": {
        "id": "_4m31QuETmip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1)\n",
        "\n",
        "Compute the posterior distribution of $\\omega$.\n",
        "\n",
        "$ω_{MAP} = \\mathop{argmax}∏\\limits_{i=1}\\limits^{n}P(ω^{(i)}|x^{(i)},y^{(i)})$\n",
        "\n",
        "Here we can apply the Bayesian Rule.\n",
        "\n",
        "$ω_{MAP} = \\mathop{argmax}∏\\limits_{i=1}\\limits^{n}\\frac{P(x{(i)},y{(i)}|ω^{(i)})g(ω)}{h(x^{(i)},y^{(i)})}$\n",
        "\n",
        "$ω_{MAP} = \\mathop{argmax}∏\\limits_{i=1}\\limits^{n}\\frac{P(y{(i)}|x{(i)},ω^{(i)})P(x^{(i)})g(ω)}{h(x^{(i)},y^{(i)})}$\n",
        "\n",
        "Then we can remove the constant term which will not influence $ω$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$ω_{MAP} = \\mathop{argmax}∏\\limits_{i=1}\\limits^{n}P(y^{(i)}|x^{(i)},ω)g(ω)$\n",
        "\n",
        "where $g(ω)$ also has a zero-mean Gaussian distribution and we assume it is distributed from $\\mathcal{N} (0,\\tau)$.\n",
        "\n",
        "$ω_{MAP}= \\mathop{argmax}∏\\limits_{i=1}\\limits^{n}\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}\\frac{1}{2πτ^2}e^{-\\frac{1}{2τ^2}(ω)^2}$\n",
        "\n"
      ],
      "metadata": {
        "id": "cSC8Ovl8TsRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2)\n",
        "\n",
        "$m(ω）= log{ω_{MAP}}$\n",
        "\n",
        "\n",
        "$m(ω) = \\mathop{argmax}\\limits_{ω}log∏\\limits_{i=1}\\limits^{n}\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}\\frac{1}{2πτ^2}e^{-\\frac{1}{2τ^2}(ω)^2}$\n",
        "\n",
        "$m(ω) = \\mathop{argmax}\\limits_{ω}\\sum\\limits_{i=1}\\limits^{n}log\\frac{1}{2πσ^2}e^{-\\frac{1}{2σ^2}(ωx^{(i)}-y^{(i)})^2}\\frac{1}{2πτ^2}e^{-\\frac{1}{2τ^2}(ω)^2}$\n",
        "\n",
        "$m(ω) = \\mathop{argmax}\\limits_{ω}{n}log\\frac{1}{2πσ^2}-{\\frac{1}{2σ^2}\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}+{n}log\\frac{1}{2πω^2}-{\\frac{1}{2τ^2}\\sum\\limits_{i=1}\\limits^{n}(ω)^2}$\n",
        "\n",
        "Cancel the constant term, since they will not influence the argmax value.\n",
        "\n",
        "$m(ω) = \\mathop{argmax}\\limits_{ω}-{\\frac{1}{2σ^2}\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}-{\\frac{1}{2τ^2}\\sum\\limits_{i=1}\\limits^{n}(ω)^2}$\n",
        "\n",
        "Since we have negative sign inside, we could write it to $argmin$\n",
        "\n",
        "$m(ω) = \\mathop{argmin}\\limits_{ω}{\\frac{1}{2σ^2}\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}+{\\frac{1}{2τ^2}\\sum\\limits_{i=1}\\limits^{n}(ω)^2}$\n",
        "\n",
        "Since $σ$ and $τ$ are constants and could effect the results, we can use a term to represent the ratio of them: $λ = \\frac{σ^2}{τ^2}$. Then we could simplify this function.\n",
        "\n",
        "$m(ω) = \\mathop{argmin}\\limits_{ω}{\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}+{λ\\sum\\limits_{i=1}\\limits^{n}(ω)^2}$\n",
        "\n",
        "So the solution is to minimize the ${\\sum\\limits_{i=1}\\limits^{n}(ωx^{(i)}-y^{(i)})^2}+{λ\\sum\\limits_{i=1}\\limits^{n}(ω)^2}$\n",
        "\n",
        "To simplify, we use $X$ and $Y$ denote the matrix form of the input $D = \\{(y^{i},x^{i})\\}_{i=1}^{n}$, where $X$ is a matrix of shape $(n,p)$, and $Y$ is a column vector with $n$ length or a matrix of shape $(n,1)$."
      ],
      "metadata": {
        "id": "mqE1pDvEi2j0"
      }
    }
  ]
}